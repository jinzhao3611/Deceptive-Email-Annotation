{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from mxnet.gluon import nn, HybridBlock, Trainer, loss\n",
    "import mxnet as mx\n",
    "from mxnet import init, autograd, ndarray as nd\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_str(string):\n",
    "    \"\"\"\n",
    "\n",
    "    :param string: a raw data string\n",
    "    :return: cleaned string\n",
    "    \"\"\"\n",
    "    string = re.sub(r'[^A-Za-z0-9(),!?\\'\\`]', ' ', string)\n",
    "    string = re.sub(r'\\s{2,}', ' ', string)\n",
    "    string = re.sub(r'\\'s', ' \\'s', string)\n",
    "    string = re.sub(r'\\'ve', ' \\'ve', string)\n",
    "    string = re.sub(r'n\\'t', ' n\\'t', string)\n",
    "    string = re.sub(r'\\'re', ' \\'re', string)\n",
    "    string = re.sub(r'\\'d', ' \\'d', string)\n",
    "    string = re.sub(r'\\'ll', ' \\'ll', string)\n",
    "    string = re.sub(r',', ' , ', string)\n",
    "    string = re.sub(r'!', ' ! ', string)\n",
    "    string = re.sub(r'\\(', ' ( ', string)\n",
    "    string = re.sub(r'\\)', ' ) ', string)\n",
    "    string = re.sub(r'\\?', ' ? ', string)\n",
    "    string = re.sub(r'\\s{2,}', ' ', string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(snts):\n",
    "    \"\"\"\n",
    "    build vocabulary from training, validation and test data\n",
    "    :param tr_array: data array\n",
    "    :param val_array: data array\n",
    "    :param tst_array: data array\n",
    "    :return: vocab in gluon\n",
    "    \"\"\"\n",
    "    flag = 0\n",
    "    all_tokens = []\n",
    "    for snt, _ in snts:\n",
    "        tokens = snt.split()\n",
    "        if len(tokens) > flag:\n",
    "            flag = len(tokens)\n",
    "        all_tokens.extend(tokens)\n",
    "    print(flag)\n",
    "    counter = nlp.data.count_tokens(all_tokens)\n",
    "    vocab = nlp.Vocab(counter)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i am miss louisa christopher the only daughter of mr christopher from republic of zimbabwe in desire to get somebody who will safe guard my interest , that of my junior brother ( louis ) and this money', 'SELF-INTRO')\n",
      "{'RAPPORT', 'BENEFITS', 'SELF-INTRO', 'MOTIVATION', 'OTHER', 'PURPOSE'}\n"
     ]
    }
   ],
   "source": [
    "file_name = \"../../gold_standard/gold_standard_final.txt\"\n",
    "snts = []\n",
    "label_set = set()\n",
    "with open(file_name, 'r') as f:\n",
    "    for row in f:\n",
    "        snt, label = row.rsplit(':', 1)\n",
    "        snts.append((_clean_str(snt.strip()), label.strip()))\n",
    "        label_set.add(label.strip())\n",
    "print((snts[0]))\n",
    "print(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RAPPORT': 0,\n",
       " 'BENEFITS': 1,\n",
       " 'SELF-INTRO': 2,\n",
       " 'MOTIVATION': 3,\n",
       " 'OTHER': 4,\n",
       " 'PURPOSE': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{label: idx for idx, label in enumerate(label_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = {'SELF-INTRO': 0, 'OTHER': 1, 'RAPPORT': 2, 'BENEFITS': 3, 'MOTIVATION': 4, 'PURPOSE': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocabulary(snts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(vocab, instance, max_snt_len=98):\n",
    "    snt, label = instance\n",
    "    word_indices = vocab.to_indices(snt.split())\n",
    "    label_index = label_set[label]\n",
    "    return word_indices, label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([36, 10, 460, 18, 15, 409, 171, 255, 6, 203, 32, 336, 11, 149, 64], 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(vocab, snts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_dataset(snts, max_snt_len=98):\n",
    "    all_data = []\n",
    "    padding = nlp.data.PadSequence(max_snt_len, pad_val=1, clip=True)  # in the vocabulary, index 1 is mapped to <pad>\n",
    "    for instance in snts:\n",
    "        word_indices, label_index = preprocess(vocab, instance)\n",
    "        all_data.append((padding(word_indices), label_index))\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = creat_dataset(snts, max_snt_len=98)\n",
    "np.random.shuffle(all_data)\n",
    "train_data = all_data[:round(len(all_data) * 0.9)]\n",
    "test_data = all_data[round(len(all_data) * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[  11  131  258  534  270   86  191   56   75  261  321  625   14  133\n",
      "   131  278  459    7   40  797  502    4  737  142  377  127    8   77\n",
      "   594  282  377  127    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1]\n",
      " [  25  135   26   58 1463 1698   64  496  767 1319   30 1228  633   28\n",
      "   649    7  285 1335 1283  715 1676  767  989    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1]\n",
      " [ 102  536 1022   22   12  194    7   27  730    6   66   13   16   15\n",
      "   253  435   80    9   11  655    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1]\n",
      " [ 508    4  512 1178 1338    6  887 1524  101   92 1296   37  101    5\n",
      "    25  622  627    8   10    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "     1    1    1    1    1    1    1    1    1    1    1    1    1    1]]\n",
      "<NDArray 4x98 @cpu(0)>\n",
      "\n",
      "[4 2 2 1]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = nlp.data.ShardedDataLoader(all_data, batch_size=4, shuffle=True)\n",
    "test_dataloader = nlp.data.ShardedDataLoader(all_data, batch_size=4, shuffle=True)\n",
    "for inputs, label in train_dataloader:\n",
    "    print(inputs)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeceptionClassifier(HybridBlock):\n",
    "\n",
    "    def __init__(self, emb_input_dim, emb_output_dim, num_classes=6, prefix=None, params=None):\n",
    "        super(DeceptionClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(emb_input_dim, emb_output_dim)\n",
    "            self.output = nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                self.output.add(nn.Dense(32))\n",
    "                self.output.add(nn.Dense(64))\n",
    "                self.output.add(nn.Dense(num_classes))\n",
    "\n",
    "    def hybrid_forward(self, F, data):\n",
    "        return self.output(self.embedding(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeceptionClassifier(emb_input_dim=len(vocab.idx_to_token), emb_output_dim=100, num_classes=len(label))\n",
    "model.initialize(init=init.Xavier(), force_reinit=True)\n",
    "# model.embedding.collect_params().setattr('grad_req', 'null')\n",
    "model.hybridize()  # OPTIONAL for efficiency - perhaps easier to comment this out during debugging\n",
    "trainer = Trainer(model.collect_params(), 'adam', {'learning_rate': 0.00025})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.9509200155735\n",
      "72.67099912464619\n",
      "65.41851350665092\n",
      "51.79095705598593\n",
      "36.14811047166586\n",
      "21.59899842273444\n",
      "12.928983259014785\n",
      "6.923688088776544\n",
      "4.8888769769109786\n",
      "3.4103482521604747\n",
      "2.4007068312494084\n",
      "1.9241070479620248\n",
      "1.5733393294503912\n",
      "1.2867386759025976\n",
      "1.0786546732997522\n",
      "0.9765101024149772\n"
     ]
    }
   ],
   "source": [
    "loss_fn = loss.SoftmaxCrossEntropyLoss()\n",
    "for epoch in range(16):\n",
    "        # begin training\n",
    "        train_loss, train_acc = 0., 0.\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            inputs = nd.array(data[0], dtype='float32')\n",
    "            labels = data[1]\n",
    "            with autograd.record(train_mode=True):\n",
    "                # the default mode is train mode\n",
    "                output = model(inputs)\n",
    "                l = loss_fn(output, labels).mean()\n",
    "            l.backward()\n",
    "            trainer.step(2)\n",
    "            train_loss += l.asscalar()\n",
    "        print(train_loss)\n",
    "#         print(evaluate(model, train_dataloader)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _acc(output, label):\n",
    "    \"\"\"\n",
    "    helper function, compute the accuracy\n",
    "    :param output: predicted result\n",
    "    :param label: golden result\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return (output.argmax(axis=1) == label.astype('float32')).mean().asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    \"\"\"\n",
    "    evaluate the result\n",
    "    :param model: trained model\n",
    "    :param dataloader:\n",
    "    :param ctx:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    labels, preds = [], []\n",
    "    for i, (data, label) in enumerate(dataloader):\n",
    "        data = nd.array(data, dtype='float32')\n",
    "        output = mx.nd.softmax(model(data))\n",
    "        pred = output.argmax(axis=1)\n",
    "        for j in range(pred.shape[0]):\n",
    "            lab = int(label[j].asscalar())\n",
    "            pre = int(pred[j].asscalar())\n",
    "            labels.append(lab)  # a list of correct labels\n",
    "            preds.append(pre)  # a list of predicted labels\n",
    "    return accuracy_score(labels, preds), precision_score(labels, preds, average='macro'), \\\n",
    "           recall_score(labels, preds, average='macro'), f1_score(labels, preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.45791245791245794,\n",
       " 0.5126703499079189,\n",
       " 0.6627906976744186,\n",
       " 0.5292913204885915)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
